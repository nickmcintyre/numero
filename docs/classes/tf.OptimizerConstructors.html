<!DOCTYPE html><html class="default"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="IE=edge"/><title>OptimizerConstructors | numero</title><meta name="description" content="Documentation for numero"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="../assets/style.css"/><link rel="stylesheet" href="../assets/highlight.css"/><script async src="../assets/search.js" id="search-script"></script></head><body><script>document.body.classList.add(localStorage.getItem("tsd-theme") || "os")</script><header><div class="tsd-page-toolbar"><div class="container"><div class="table-wrap"><div class="table-cell" id="tsd-search" data-base=".."><div class="field"><label for="tsd-search-field" class="tsd-widget search no-caption">Search</label><input type="text" id="tsd-search-field"/></div><ul class="results"><li class="state loading">Preparing search index...</li><li class="state failure">The search index is not available</li></ul><a href="../index.html" class="title">numero</a></div><div class="table-cell" id="tsd-widgets"><div id="tsd-filter"><a href="#" class="tsd-widget options no-caption" data-toggle="options">Options</a><div class="tsd-filter-group"><div class="tsd-select" id="tsd-filter-visibility"><span class="tsd-select-label">All</span><ul class="tsd-select-list"><li data-value="public">Public</li><li data-value="protected">Public/Protected</li><li data-value="private" class="selected">All</li></ul></div> <input type="checkbox" id="tsd-filter-inherited" checked/><label class="tsd-widget" for="tsd-filter-inherited">Inherited</label><input type="checkbox" id="tsd-filter-externals" checked/><label class="tsd-widget" for="tsd-filter-externals">Externals</label></div></div><a href="#" class="tsd-widget menu no-caption" data-toggle="menu">Menu</a></div></div></div></div><div class="tsd-page-title"><div class="container"><ul class="tsd-breadcrumb"><li><a href="../modules.html">numero</a></li><li><a href="../modules/tf.html">tf</a></li><li><a href="tf.OptimizerConstructors.html">OptimizerConstructors</a></li></ul><h1>Class OptimizerConstructors</h1></div></div></header><div class="container container-main"><div class="row"><div class="col-8 col-content"><section class="tsd-panel tsd-hierarchy"><h3>Hierarchy</h3><ul class="tsd-hierarchy"><li><span class="target">OptimizerConstructors</span></li></ul></section><section class="tsd-panel-group tsd-index-group"><h2>Index</h2><section class="tsd-panel tsd-index-panel"><div class="tsd-index-content"><section class="tsd-index-section tsd-is-external"><h3>Constructors</h3><ul class="tsd-index-list"><li class="tsd-kind-constructor tsd-parent-kind-class tsd-is-external"><a href="tf.OptimizerConstructors.html#constructor" class="tsd-kind-icon">constructor</a></li></ul></section><section class="tsd-index-section tsd-is-external"><h3>Methods</h3><ul class="tsd-index-list"><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#adadelta" class="tsd-kind-icon">adadelta</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#adagrad" class="tsd-kind-icon">adagrad</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#adam" class="tsd-kind-icon">adam</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#adamax" class="tsd-kind-icon">adamax</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#momentum" class="tsd-kind-icon">momentum</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#rmsprop" class="tsd-kind-icon">rmsprop</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#sgd" class="tsd-kind-icon">sgd</a></li></ul></section></div></section></section><section class="tsd-panel-group tsd-member-group tsd-is-external"><h2>Constructors</h2><section class="tsd-panel tsd-member tsd-kind-constructor tsd-parent-kind-class tsd-is-external"><a id="constructor" class="tsd-anchor"></a><h3 class="tsd-anchor-link">constructor<a href="#constructor" aria-label="Permalink" class="tsd-anchor-icon"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5"></path><path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5"></path></svg></a></h3><ul class="tsd-signatures tsd-kind-constructor tsd-parent-kind-class tsd-is-external"><li class="tsd-signature tsd-kind-icon">new <wbr/>Optimizer<wbr/>Constructors<span class="tsd-signature-symbol">(</span><span class="tsd-signature-symbol">)</span><span class="tsd-signature-symbol">: </span><a href="tf.OptimizerConstructors.html" class="tsd-signature-type" data-tsd-kind="Class">OptimizerConstructors</a></li></ul><ul class="tsd-descriptions"><li class="tsd-description"><h4 class="tsd-returns-title">Returns <a href="tf.OptimizerConstructors.html" class="tsd-signature-type" data-tsd-kind="Class">OptimizerConstructors</a></h4></li></ul></section></section><section class="tsd-panel-group tsd-member-group tsd-is-external"><h2>Methods</h2><section class="tsd-panel tsd-member tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a id="adadelta" class="tsd-anchor"></a><h3 class="tsd-anchor-link"><span class="tsd-flag ts-flagStatic">Static</span> adadelta<a href="#adadelta" aria-label="Permalink" class="tsd-anchor-icon"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5"></path><path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5"></path></svg></a></h3><ul class="tsd-signatures tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><li class="tsd-signature tsd-kind-icon">adadelta<span class="tsd-signature-symbol">(</span>learningRate<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, rho<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, epsilon<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span><span class="tsd-signature-symbol">)</span><span class="tsd-signature-symbol">: </span><a href="tf.AdadeltaOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">AdadeltaOptimizer</a></li></ul><ul class="tsd-descriptions"><li class="tsd-description"><aside class="tsd-sources"><ul><li>Defined in node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.d.ts:126</li></ul></aside><div class="tsd-comment tsd-typography"><div class="lead">
<p>Constructs a <code>tf.AdadeltaOptimizer</code> that uses the Adadelta algorithm.
See <a href="https://arxiv.org/abs/1212.5701">https://arxiv.org/abs/1212.5701</a></p>
</div><dl class="tsd-comment-tags"><dt>doc</dt><dd><p>{heading: &#39;Training&#39;, subheading: &#39;Optimizers&#39;, namespace: &#39;train&#39;}</p>
</dd></dl></div><h4 class="tsd-parameters-title">Parameters</h4><ul class="tsd-parameters"><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> learningRate: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate to use for the Adadelta gradient
descent algorithm.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> rho: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate decay over each update.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> epsilon: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>A constant epsilon used to better condition the grad
update.</p>
</div></div></li></ul><h4 class="tsd-returns-title">Returns <a href="tf.AdadeltaOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">AdadeltaOptimizer</a></h4></li></ul></section><section class="tsd-panel tsd-member tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a id="adagrad" class="tsd-anchor"></a><h3 class="tsd-anchor-link"><span class="tsd-flag ts-flagStatic">Static</span> adagrad<a href="#adagrad" aria-label="Permalink" class="tsd-anchor-icon"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5"></path><path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5"></path></svg></a></h3><ul class="tsd-signatures tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><li class="tsd-signature tsd-kind-icon">adagrad<span class="tsd-signature-symbol">(</span>learningRate<span class="tsd-signature-symbol">: </span><span class="tsd-signature-type">number</span>, initialAccumulatorValue<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span><span class="tsd-signature-symbol">)</span><span class="tsd-signature-symbol">: </span><a href="tf.AdagradOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">AdagradOptimizer</a></li></ul><ul class="tsd-descriptions"><li class="tsd-description"><aside class="tsd-sources"><ul><li>Defined in node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.d.ts:157</li></ul></aside><div class="tsd-comment tsd-typography"><div class="lead">
<p>Constructs a <code>tf.AdagradOptimizer</code> that uses the Adagrad algorithm.
See
<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a>
or
<a href="http://ruder.io/optimizing-gradient-descent/index.html#adagrad">http://ruder.io/optimizing-gradient-descent/index.html#adagrad</a></p>
</div><dl class="tsd-comment-tags"><dt>doc</dt><dd><p>{heading: &#39;Training&#39;, subheading: &#39;Optimizers&#39;, namespace: &#39;train&#39;}</p>
</dd></dl></div><h4 class="tsd-parameters-title">Parameters</h4><ul class="tsd-parameters"><li><h5>learningRate: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate to use for the Adagrad gradient
descent algorithm.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> initialAccumulatorValue: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>Starting value for the accumulators, must be
positive.</p>
</div></div></li></ul><h4 class="tsd-returns-title">Returns <a href="tf.AdagradOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">AdagradOptimizer</a></h4></li></ul></section><section class="tsd-panel tsd-member tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a id="adam" class="tsd-anchor"></a><h3 class="tsd-anchor-link"><span class="tsd-flag ts-flagStatic">Static</span> adam<a href="#adam" aria-label="Permalink" class="tsd-anchor-icon"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5"></path><path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5"></path></svg></a></h3><ul class="tsd-signatures tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><li class="tsd-signature tsd-kind-icon">adam<span class="tsd-signature-symbol">(</span>learningRate<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, beta1<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, beta2<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, epsilon<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span><span class="tsd-signature-symbol">)</span><span class="tsd-signature-symbol">: </span><a href="tf.AdamOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">AdamOptimizer</a></li></ul><ul class="tsd-descriptions"><li class="tsd-description"><aside class="tsd-sources"><ul><li>Defined in node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.d.ts:113</li></ul></aside><div class="tsd-comment tsd-typography"><div class="lead">
<p>Constructs a <code>tf.AdamOptimizer</code> that uses the Adam algorithm.
See <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a></p>
</div><dl class="tsd-comment-tags"><dt>doc</dt><dd><p>{heading: &#39;Training&#39;, subheading: &#39;Optimizers&#39;, namespace: &#39;train&#39;}</p>
</dd></dl></div><h4 class="tsd-parameters-title">Parameters</h4><ul class="tsd-parameters"><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> learningRate: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate to use for the Adam gradient
descent algorithm.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> beta1: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The exponential decay rate for the 1st moment estimates.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> beta2: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The exponential decay rate for the 2nd moment estimates.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> epsilon: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>A small constant for numerical stability.</p>
</div></div></li></ul><h4 class="tsd-returns-title">Returns <a href="tf.AdamOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">AdamOptimizer</a></h4></li></ul></section><section class="tsd-panel tsd-member tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a id="adamax" class="tsd-anchor"></a><h3 class="tsd-anchor-link"><span class="tsd-flag ts-flagStatic">Static</span> adamax<a href="#adamax" aria-label="Permalink" class="tsd-anchor-icon"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5"></path><path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5"></path></svg></a></h3><ul class="tsd-signatures tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><li class="tsd-signature tsd-kind-icon">adamax<span class="tsd-signature-symbol">(</span>learningRate<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, beta1<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, beta2<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, epsilon<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, decay<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span><span class="tsd-signature-symbol">)</span><span class="tsd-signature-symbol">: </span><a href="tf.AdamaxOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">AdamaxOptimizer</a></li></ul><ul class="tsd-descriptions"><li class="tsd-description"><aside class="tsd-sources"><ul><li>Defined in node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.d.ts:140</li></ul></aside><div class="tsd-comment tsd-typography"><div class="lead">
<p>Constructs a <code>tf.AdamaxOptimizer</code> that uses the Adamax algorithm.
See <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a></p>
</div><dl class="tsd-comment-tags"><dt>doc</dt><dd><p>{heading: &#39;Training&#39;, subheading: &#39;Optimizers&#39;, namespace: &#39;train&#39;}</p>
</dd></dl></div><h4 class="tsd-parameters-title">Parameters</h4><ul class="tsd-parameters"><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> learningRate: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate to use for the Adamax gradient
descent algorithm.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> beta1: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The exponential decay rate for the 1st moment estimates.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> beta2: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The exponential decay rate for the 2nd moment estimates.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> epsilon: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>A small constant for numerical stability.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> decay: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate decay over each update.</p>
</div></div></li></ul><h4 class="tsd-returns-title">Returns <a href="tf.AdamaxOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">AdamaxOptimizer</a></h4></li></ul></section><section class="tsd-panel tsd-member tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a id="momentum" class="tsd-anchor"></a><h3 class="tsd-anchor-link"><span class="tsd-flag ts-flagStatic">Static</span> momentum<a href="#momentum" aria-label="Permalink" class="tsd-anchor-icon"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5"></path><path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5"></path></svg></a></h3><ul class="tsd-signatures tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><li class="tsd-signature tsd-kind-icon">momentum<span class="tsd-signature-symbol">(</span>learningRate<span class="tsd-signature-symbol">: </span><span class="tsd-signature-type">number</span>, momentum<span class="tsd-signature-symbol">: </span><span class="tsd-signature-type">number</span>, useNesterov<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">boolean</span><span class="tsd-signature-symbol">)</span><span class="tsd-signature-symbol">: </span><a href="tf.MomentumOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">MomentumOptimizer</a></li></ul><ul class="tsd-descriptions"><li class="tsd-description"><aside class="tsd-sources"><ul><li>Defined in node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.d.ts:79</li></ul></aside><div class="tsd-comment tsd-typography"><div class="lead">
<p>Constructs a <code>tf.MomentumOptimizer</code> that uses momentum gradient
descent.</p>
</div><div><p>See
<a href="http://proceedings.mlr.press/v28/sutskever13.pdf">http://proceedings.mlr.press/v28/sutskever13.pdf</a></p>
</div><dl class="tsd-comment-tags"><dt>doc</dt><dd><p>{heading: &#39;Training&#39;, subheading: &#39;Optimizers&#39;, namespace: &#39;train&#39;}</p>
</dd></dl></div><h4 class="tsd-parameters-title">Parameters</h4><ul class="tsd-parameters"><li><h5>learningRate: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate to use for the Momentum gradient
descent algorithm.</p>
</div></div></li><li><h5>momentum: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The momentum to use for the momentum gradient descent
algorithm.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> useNesterov: <span class="tsd-signature-type">boolean</span></h5></li></ul><h4 class="tsd-returns-title">Returns <a href="tf.MomentumOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">MomentumOptimizer</a></h4></li></ul></section><section class="tsd-panel tsd-member tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a id="rmsprop" class="tsd-anchor"></a><h3 class="tsd-anchor-link"><span class="tsd-flag ts-flagStatic">Static</span> rmsprop<a href="#rmsprop" aria-label="Permalink" class="tsd-anchor-icon"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5"></path><path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5"></path></svg></a></h3><ul class="tsd-signatures tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><li class="tsd-signature tsd-kind-icon">rmsprop<span class="tsd-signature-symbol">(</span>learningRate<span class="tsd-signature-symbol">: </span><span class="tsd-signature-type">number</span>, decay<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, momentum<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, epsilon<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">number</span>, centered<span class="tsd-signature-symbol">?: </span><span class="tsd-signature-type">boolean</span><span class="tsd-signature-symbol">)</span><span class="tsd-signature-symbol">: </span><a href="tf.RMSPropOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">RMSPropOptimizer</a></li></ul><ul class="tsd-descriptions"><li class="tsd-description"><aside class="tsd-sources"><ul><li>Defined in node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.d.ts:100</li></ul></aside><div class="tsd-comment tsd-typography"><div class="lead">
<p>Constructs a <code>tf.RMSPropOptimizer</code> that uses RMSProp gradient
descent. This implementation uses plain momentum and is not centered
version of RMSProp.</p>
</div><div><p>See
<a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></p>
</div><dl class="tsd-comment-tags"><dt>doc</dt><dd><p>{heading: &#39;Training&#39;, subheading: &#39;Optimizers&#39;, namespace: &#39;train&#39;}</p>
</dd></dl></div><h4 class="tsd-parameters-title">Parameters</h4><ul class="tsd-parameters"><li><h5>learningRate: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate to use for the RMSProp gradient
descent algorithm.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> decay: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The discounting factor for the history/coming gradient.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> momentum: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The momentum to use for the RMSProp gradient descent
algorithm.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> epsilon: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>Small value to avoid zero denominator.</p>
</div></div></li><li><h5><span class="tsd-flag ts-flagOptional">Optional</span> centered: <span class="tsd-signature-type">boolean</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>If true, gradients are normalized by the estimated
variance of the gradient.</p>
</div></div></li></ul><h4 class="tsd-returns-title">Returns <a href="tf.RMSPropOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">RMSPropOptimizer</a></h4></li></ul></section><section class="tsd-panel tsd-member tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a id="sgd" class="tsd-anchor"></a><h3 class="tsd-anchor-link"><span class="tsd-flag ts-flagStatic">Static</span> sgd<a href="#sgd" aria-label="Permalink" class="tsd-anchor-icon"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5"></path><path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5"></path></svg></a></h3><ul class="tsd-signatures tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><li class="tsd-signature tsd-kind-icon">sgd<span class="tsd-signature-symbol">(</span>learningRate<span class="tsd-signature-symbol">: </span><span class="tsd-signature-type">number</span><span class="tsd-signature-symbol">)</span><span class="tsd-signature-symbol">: </span><a href="tf.SGDOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">SGDOptimizer</a></li></ul><ul class="tsd-descriptions"><li class="tsd-description"><aside class="tsd-sources"><ul><li>Defined in node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.d.ts:63</li></ul></aside><div class="tsd-comment tsd-typography"><div class="lead">
<p>Constructs a <code>tf.SGDOptimizer</code> that uses stochastic gradient descent.</p>
</div><div><pre><code class="language-js"><span class="hl-0">// Fit a quadratic function by learning the coefficients a, b, c.</span><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-3">xs</span><span class="hl-2"> = </span><span class="hl-4">tf</span><span class="hl-2">.</span><span class="hl-5">tensor1d</span><span class="hl-2">([</span><span class="hl-6">0</span><span class="hl-2">, </span><span class="hl-6">1</span><span class="hl-2">, </span><span class="hl-6">2</span><span class="hl-2">, </span><span class="hl-6">3</span><span class="hl-2">]);</span><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-3">ys</span><span class="hl-2"> = </span><span class="hl-4">tf</span><span class="hl-2">.</span><span class="hl-5">tensor1d</span><span class="hl-2">([</span><span class="hl-6">1.1</span><span class="hl-2">, </span><span class="hl-6">5.9</span><span class="hl-2">, </span><span class="hl-6">16.8</span><span class="hl-2">, </span><span class="hl-6">33.9</span><span class="hl-2">]);</span><br/><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-3">a</span><span class="hl-2"> = </span><span class="hl-4">tf</span><span class="hl-2">.</span><span class="hl-5">scalar</span><span class="hl-2">(</span><span class="hl-10">Math</span><span class="hl-2">.</span><span class="hl-5">random</span><span class="hl-2">()).</span><span class="hl-5">variable</span><span class="hl-2">();</span><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-3">b</span><span class="hl-2"> = </span><span class="hl-4">tf</span><span class="hl-2">.</span><span class="hl-5">scalar</span><span class="hl-2">(</span><span class="hl-10">Math</span><span class="hl-2">.</span><span class="hl-5">random</span><span class="hl-2">()).</span><span class="hl-5">variable</span><span class="hl-2">();</span><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-3">c</span><span class="hl-2"> = </span><span class="hl-4">tf</span><span class="hl-2">.</span><span class="hl-5">scalar</span><span class="hl-2">(</span><span class="hl-10">Math</span><span class="hl-2">.</span><span class="hl-5">random</span><span class="hl-2">()).</span><span class="hl-5">variable</span><span class="hl-2">();</span><br/><br/><span class="hl-0">// y = a * x^2 + b * x + c.</span><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-5">f</span><span class="hl-2"> = </span><span class="hl-4">x</span><span class="hl-2"> </span><span class="hl-1">=&gt;</span><span class="hl-2"> </span><span class="hl-4">a</span><span class="hl-2">.</span><span class="hl-5">mul</span><span class="hl-2">(</span><span class="hl-4">x</span><span class="hl-2">.</span><span class="hl-5">square</span><span class="hl-2">()).</span><span class="hl-5">add</span><span class="hl-2">(</span><span class="hl-4">b</span><span class="hl-2">.</span><span class="hl-5">mul</span><span class="hl-2">(</span><span class="hl-4">x</span><span class="hl-2">)).</span><span class="hl-5">add</span><span class="hl-2">(</span><span class="hl-4">c</span><span class="hl-2">);</span><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-5">loss</span><span class="hl-2"> = (</span><span class="hl-4">pred</span><span class="hl-2">, </span><span class="hl-4">label</span><span class="hl-2">) </span><span class="hl-1">=&gt;</span><span class="hl-2"> </span><span class="hl-4">pred</span><span class="hl-2">.</span><span class="hl-5">sub</span><span class="hl-2">(</span><span class="hl-4">label</span><span class="hl-2">).</span><span class="hl-5">square</span><span class="hl-2">().</span><span class="hl-5">mean</span><span class="hl-2">();</span><br/><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-3">learningRate</span><span class="hl-2"> = </span><span class="hl-6">0.01</span><span class="hl-2">;</span><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-3">optimizer</span><span class="hl-2"> = </span><span class="hl-4">tf</span><span class="hl-2">.</span><span class="hl-4">train</span><span class="hl-2">.</span><span class="hl-5">sgd</span><span class="hl-2">(</span><span class="hl-4">learningRate</span><span class="hl-2">);</span><br/><br/><span class="hl-0">// Train the model.</span><br/><span class="hl-8">for</span><span class="hl-2"> (</span><span class="hl-1">let</span><span class="hl-2"> </span><span class="hl-4">i</span><span class="hl-2"> = </span><span class="hl-6">0</span><span class="hl-2">; </span><span class="hl-4">i</span><span class="hl-2"> &lt; </span><span class="hl-6">10</span><span class="hl-2">; </span><span class="hl-4">i</span><span class="hl-2">++) {</span><br/><span class="hl-2">  </span><span class="hl-4">optimizer</span><span class="hl-2">.</span><span class="hl-5">minimize</span><span class="hl-2">(() </span><span class="hl-1">=&gt;</span><span class="hl-2"> </span><span class="hl-5">loss</span><span class="hl-2">(</span><span class="hl-5">f</span><span class="hl-2">(</span><span class="hl-4">xs</span><span class="hl-2">), </span><span class="hl-4">ys</span><span class="hl-2">));</span><br/><span class="hl-2">}</span><br/><br/><span class="hl-0">// Make predictions.</span><br/><span class="hl-4">console</span><span class="hl-2">.</span><span class="hl-5">log</span><span class="hl-2">(</span><br/><span class="hl-2">    </span><span class="hl-7">`a: </span><span class="hl-1">${</span><span class="hl-4">a</span><span class="hl-9">.</span><span class="hl-5">dataSync</span><span class="hl-9">()</span><span class="hl-1">}</span><span class="hl-7">, b: </span><span class="hl-1">${</span><span class="hl-4">b</span><span class="hl-9">.</span><span class="hl-5">dataSync</span><span class="hl-9">()</span><span class="hl-1">}</span><span class="hl-7">, c: </span><span class="hl-1">${</span><span class="hl-4">c</span><span class="hl-9">.</span><span class="hl-5">dataSync</span><span class="hl-9">()</span><span class="hl-1">}</span><span class="hl-7">`</span><span class="hl-2">);</span><br/><span class="hl-1">const</span><span class="hl-2"> </span><span class="hl-3">preds</span><span class="hl-2"> = </span><span class="hl-5">f</span><span class="hl-2">(</span><span class="hl-4">xs</span><span class="hl-2">).</span><span class="hl-5">dataSync</span><span class="hl-2">();</span><br/><span class="hl-4">preds</span><span class="hl-2">.</span><span class="hl-5">forEach</span><span class="hl-2">((</span><span class="hl-4">pred</span><span class="hl-2">, </span><span class="hl-4">i</span><span class="hl-2">) </span><span class="hl-1">=&gt;</span><span class="hl-2"> {</span><br/><span class="hl-2">  </span><span class="hl-4">console</span><span class="hl-2">.</span><span class="hl-5">log</span><span class="hl-2">(</span><span class="hl-7">`x: </span><span class="hl-1">${</span><span class="hl-4">i</span><span class="hl-1">}</span><span class="hl-7">, pred: </span><span class="hl-1">${</span><span class="hl-4">pred</span><span class="hl-1">}</span><span class="hl-7">`</span><span class="hl-2">);</span><br/><span class="hl-2">});</span>
</code></pre>
</div><dl class="tsd-comment-tags"><dt>doc</dt><dd><p>{heading: &#39;Training&#39;, subheading: &#39;Optimizers&#39;, namespace: &#39;train&#39;}</p>
</dd></dl></div><h4 class="tsd-parameters-title">Parameters</h4><ul class="tsd-parameters"><li><h5>learningRate: <span class="tsd-signature-type">number</span></h5><div class="tsd-comment tsd-typography"><div class="lead">
<p>The learning rate to use for the SGD algorithm.</p>
</div></div></li></ul><h4 class="tsd-returns-title">Returns <a href="tf.SGDOptimizer.html" class="tsd-signature-type" data-tsd-kind="Class">SGDOptimizer</a></h4></li></ul></section></section></div><div class="col-4 col-menu menu-sticky-wrap menu-highlight"><nav class="tsd-navigation primary"><ul><li class=""><a href="../modules.html">Exports</a></li><li class="label tsd-is-external"><span>Internals</span></li><li class="label tsd-is-external"><span>Externals</span></li><li class=" tsd-kind-namespace tsd-is-external"><a href="../modules/math.html">math</a></li><li class="current tsd-kind-namespace tsd-is-external"><a href="../modules/tf.html">tf</a><ul><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.Tensor.html">Tensor</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.backend_util.html">backend_<wbr/>util</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.broadcast_util.html">broadcast_<wbr/>util</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.browser.html">browser</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.constraints.html">constraints</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.data.html">data</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.device_util.html">device_<wbr/>util</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.fused.html">fused</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.gather_util.html">gather_<wbr/>util</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.initializers.html">initializers</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.io.html">io</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.kernel_impls.html">kernel_<wbr/>impls</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.layers.html">layers</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.math.html">math</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.metrics.html">metrics</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.models.html">models</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.regularizers.html">regularizers</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.scatter_util.html">scatter_<wbr/>util</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.serialization.html">serialization</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.tensor_util.html">tensor_<wbr/>util</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.test_util.html">test_<wbr/>util</a></li><li class=" tsd-kind-namespace tsd-parent-kind-namespace tsd-is-external"><a href="../modules/tf.util.html">util</a></li></ul></li></ul></nav><nav class="tsd-navigation secondary menu-sticky"><ul><li class="current tsd-kind-class tsd-parent-kind-namespace tsd-is-external"><a href="tf.OptimizerConstructors.html" class="tsd-kind-icon">Optimizer<wbr/>Constructors</a><ul><li class="tsd-kind-constructor tsd-parent-kind-class tsd-is-external"><a href="tf.OptimizerConstructors.html#constructor" class="tsd-kind-icon">constructor</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#adadelta" class="tsd-kind-icon">adadelta</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#adagrad" class="tsd-kind-icon">adagrad</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#adam" class="tsd-kind-icon">adam</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#adamax" class="tsd-kind-icon">adamax</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#momentum" class="tsd-kind-icon">momentum</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#rmsprop" class="tsd-kind-icon">rmsprop</a></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static tsd-is-external"><a href="tf.OptimizerConstructors.html#sgd" class="tsd-kind-icon">sgd</a></li></ul></li></ul></nav></div></div></div><footer class="with-border-bottom"><div class="container"><h2>Legend</h2><div class="tsd-legend-group"><ul class="tsd-legend"><li class="tsd-kind-constructor tsd-parent-kind-class"><span class="tsd-kind-icon">Constructor</span></li><li class="tsd-kind-property tsd-parent-kind-class"><span class="tsd-kind-icon">Property</span></li><li class="tsd-kind-method tsd-parent-kind-class"><span class="tsd-kind-icon">Method</span></li></ul><ul class="tsd-legend"><li class="tsd-kind-property tsd-parent-kind-class tsd-is-private"><span class="tsd-kind-icon">Private property</span></li><li class="tsd-kind-method tsd-parent-kind-class tsd-is-private"><span class="tsd-kind-icon">Private method</span></li></ul><ul class="tsd-legend"><li class="tsd-kind-method tsd-parent-kind-class tsd-is-static"><span class="tsd-kind-icon">Static method</span></li></ul></div><h2>Settings</h2><p>Theme <select id="theme"><option value="os">OS</option><option value="light">Light</option><option value="dark">Dark</option></select></p></div></footer><div class="container tsd-generator"><p>Generated using <a href="https://typedoc.org/" target="_blank">TypeDoc</a></p></div><div class="overlay"></div><script src="../assets/main.js"></script></body></html>